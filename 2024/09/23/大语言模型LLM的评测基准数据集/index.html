<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大语言模型LLM的评测基准数据集 | daxiong's blog</title><meta name="author" content="daxiong"><meta name="copyright" content="daxiong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="llm_benchmarksA collection of benchmarks and datasets for evaluating LLM. Knowledge and Language UnderstandingMassive Multitask Language Understanding (MMLU) Description: Measures general knowledge ac">
<meta property="og:type" content="article">
<meta property="og:title" content="大语言模型LLM的评测基准数据集">
<meta property="og:url" content="https://88daxiong.github.io/2024/09/23/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BLLM%E7%9A%84%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86/index.html">
<meta property="og:site_name" content="daxiong&#39;s blog">
<meta property="og:description" content="llm_benchmarksA collection of benchmarks and datasets for evaluating LLM. Knowledge and Language UnderstandingMassive Multitask Language Understanding (MMLU) Description: Measures general knowledge ac">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://88daxiong.github.io/image/bald_eagle.jpeg">
<meta property="article:published_time" content="2024-09-23T02:30:15.000Z">
<meta property="article:modified_time" content="2024-09-23T03:17:56.757Z">
<meta property="article:author" content="daxiong">
<meta property="article:tag" content="llm">
<meta property="article:tag" content="评测">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://88daxiong.github.io/image/bald_eagle.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://88daxiong.github.io/2024/09/23/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BLLM%E7%9A%84%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大语言模型LLM的评测基准数据集',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-23 11:17:56'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><script src="https://npm.elemecdn.com/echarts@4.9.0/map/js/china.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/image/bald_eagle.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="daxiong's blog"><span class="site-name">daxiong's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大语言模型LLM的评测基准数据集</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-23T02:30:15.000Z" title="发表于 2024-09-23 10:30:15">2024-09-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-23T03:17:56.757Z" title="更新于 2024-09-23 11:17:56">2024-09-23</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大语言模型LLM的评测基准数据集"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="llm-benchmarks"><a href="#llm-benchmarks" class="headerlink" title="llm_benchmarks"></a>llm_benchmarks</h1><p>A collection of benchmarks and datasets for evaluating LLM.</p>
<h2 id="Knowledge-and-Language-Understanding"><a href="#Knowledge-and-Language-Understanding" class="headerlink" title="Knowledge and Language Understanding"></a>Knowledge and Language Understanding</h2><h3 id="Massive-Multitask-Language-Understanding-MMLU"><a href="#Massive-Multitask-Language-Understanding-MMLU" class="headerlink" title="Massive Multitask Language Understanding (MMLU)"></a>Massive Multitask Language Understanding (MMLU)</h3><ul>
<li><strong>Description:</strong> Measures general knowledge across 57 different subjects, ranging from STEM to social sciences.</li>
<li><strong>Purpose:</strong> To assess the LLM’s understanding and reasoning in a wide range of subject areas.</li>
<li><strong>Relevance:</strong> Ideal for multifaceted AI systems that require extensive world knowledge and problem solving ability.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.03300">Measuring Massive Multitask Language Understanding</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/hendrycks/test">MMLU GitHub</a></li>
<li><a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~hendrycks/data.tar">MMLU Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="AI2-Reasoning-Challenge-ARC"><a href="#AI2-Reasoning-Challenge-ARC" class="headerlink" title="AI2 Reasoning Challenge (ARC)"></a>AI2 Reasoning Challenge (ARC)</h3><ul>
<li><strong>Description:</strong> Tests LLMs on grade-school science questions, requiring both deep general knowledge and reasoning abilities.</li>
<li><strong>Purpose:</strong> To evaluate the ability to answer complex science questions that require logical reasoning.</li>
<li><strong>Relevance:</strong> Useful for educational AI applications, automated tutoring systems, and general knowledge assessments.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.05457">Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ai2_arc">ARC Dataset: HuggingFace</a></li>
<li><a target="_blank" rel="noopener" href="https://allenai.org/data/arc">ARC Dataset: Allen Institute</a></li>
</ul>
</li>
</ul>
<h3 id="General-Language-Understanding-Evaluation-GLUE"><a href="#General-Language-Understanding-Evaluation-GLUE" class="headerlink" title="General Language Understanding Evaluation (GLUE)"></a>General Language Understanding Evaluation (GLUE)</h3><ul>
<li><strong>Description:</strong> A collection of various language tasks from multiple datasets, designed to measure overall language understanding.</li>
<li><strong>Purpose:</strong> To provide a comprehensive assessment of language understanding abilities in different contexts.</li>
<li><strong>Relevance:</strong> Crucial for applications requiring advanced language processing, such as chatbots and content analysis.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.07461">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://gluebenchmark.com/">GLUE Homepage</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/glue">GLUE Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="Natural-Questions"><a href="#Natural-Questions" class="headerlink" title="Natural Questions"></a>Natural Questions</h3><ul>
<li><strong>Description:</strong> A collection of real-world questions people have Googled, paired with relevant Wikipedia pages to extract answers.</li>
<li><strong>Purpose:</strong> To test the ability to find accurate short and long answers from web-based sources.</li>
<li><strong>Relevance:</strong> Essential for search engines, information retrieval systems, and AI-driven question-answering tools.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00276/43518/Natural-Questions-A-Benchmark-for-Question">Natural Questions: A Benchmark for Question Answering Research</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://ai.google.com/research/NaturalQuestions">Natural Questions Homepage</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/google-research-datasets/natural-questions">Natural Questions Dataset: Github</a></li>
</ul>
</li>
</ul>
<h3 id="LAnguage-Modelling-Broadened-to-Account-for-Discourse-Aspects-LAMBADA"><a href="#LAnguage-Modelling-Broadened-to-Account-for-Discourse-Aspects-LAMBADA" class="headerlink" title="LAnguage Modelling Broadened to Account for Discourse Aspects (LAMBADA)"></a>LAnguage Modelling Broadened to Account for Discourse Aspects (LAMBADA)</h3><ul>
<li><strong>Description:</strong> A collection of passages testing the ability of language models to understand and predict text based on long-range context.</li>
<li><strong>Purpose:</strong> To assess the models’ comprehension of narratives and their predictive abilities in text generation.</li>
<li><strong>Relevance:</strong> Important for AI applications in narrative analysis, content creation, and long-form text understanding.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.06031">The LAMBADA Dataset: Word prediction requiring a broad discourse context</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/lambada">LAMBADA Dataset: HuggingFace</a></li>
</ul>
</li>
</ul>
<h3 id="HellaSwag"><a href="#HellaSwag" class="headerlink" title="HellaSwag"></a>HellaSwag</h3><ul>
<li><strong>Description:</strong> Tests natural language inference by requiring LLMs to complete passages in a way that requires understanding intricate details.</li>
<li><strong>Purpose:</strong> To evaluate the model’s ability to generate contextually appropriate text continuations.</li>
<li><strong>Relevance:</strong> Useful in content creation, dialogue systems, and applications requiring advanced text generation capabilities.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.07830">HellaSwag: Can a Machine Really Finish Your Sentence?</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/rowanz/hellaswag/tree/master/data">HellaSwag Dataset: GitHub</a></li>
</ul>
</li>
</ul>
<h3 id="Multi-Genre-Natural-Language-Inference-MultiNLI"><a href="#Multi-Genre-Natural-Language-Inference-MultiNLI" class="headerlink" title="Multi-Genre Natural Language Inference (MultiNLI)"></a>Multi-Genre Natural Language Inference (MultiNLI)</h3><ul>
<li><strong>Description:</strong> A benchmark consisting of 433K sentence pairs across various genres of English data, testing natural language inference.</li>
<li><strong>Purpose:</strong> To assess the ability of LLMs to assign correct labels to hypothesis statements based on premises.</li>
<li><strong>Relevance:</strong> Vital for systems requiring advanced text comprehension and inference, such as automated reasoning and text analytics tools.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.05426">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://cims.nyu.edu/~sbowman/multinli/">MultiNLI Homepage</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/multi_nli">MultiNLI Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="SuperGLUE"><a href="#SuperGLUE" class="headerlink" title="SuperGLUE"></a>SuperGLUE</h3><ul>
<li><strong>Description:</strong> An advanced version of the GLUE benchmark, comprising more challenging and diverse language tasks.</li>
<li><strong>Purpose:</strong> To evaluate deeper aspects of language understanding and reasoning.</li>
<li><strong>Relevance:</strong> Important for sophisticated AI systems requiring advanced language processing capabilities.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.00537">SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/super_glue">SuperGLUE Dataset: HuggingFace</a></li>
</ul>
</li>
</ul>
<h3 id="TriviaQA"><a href="#TriviaQA" class="headerlink" title="TriviaQA"></a>TriviaQA</h3><ul>
<li><strong>Description:</strong> A reading comprehension test with questions from sources like Wikipedia, demanding contextual analysis.</li>
<li><strong>Purpose:</strong> To assess the ability to sift through context and find accurate answers in complex texts.</li>
<li><strong>Relevance:</strong> Suitable for AI systems in knowledge extraction, research, and detailed content analysis.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.03551">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/mandarjoshi90/triviaqa">TriviaQA GitHub</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/trivia_qa">TriviaQa Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="WinoGrande"><a href="#WinoGrande" class="headerlink" title="WinoGrande"></a>WinoGrande</h3><ul>
<li><strong>Description:</strong> A large set of problems based on the Winograd Schema Challenge, testing context understanding in sentences.</li>
<li><strong>Purpose:</strong> To evaluate the ability of LLMs to grasp nuanced context and subtle variations in text.</li>
<li><strong>Relevance:</strong> Crucial for models dealing with narrative analysis, content personalization, and advanced text interpretation.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.10641">WinoGrande: An Adversarial Winograd Schema Challenge at Scale</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/allenai/winogrande">WinoGrande GitHub</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/winogrande">WinoGrande Dataset: HuggingFace</a></li>
</ul>
</li>
</ul>
<h3 id="SciQ"><a href="#SciQ" class="headerlink" title="SciQ"></a>SciQ</h3><ul>
<li><strong>Description:</strong> Consists of multiple-choice questions mainly in natural sciences like physics, chemistry, and biology.</li>
<li><strong>Purpose:</strong> To test the ability to answer science-based questions, often with additional supporting text.</li>
<li><strong>Relevance:</strong> Useful for educational tools, especially in science education and knowledge testing platforms.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.06209">Crowdsourcing Multiple Choice Science Questions</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/sciq">SciQ Dataset: HuggingFace</a></li>
</ul>
</li>
</ul>
<h2 id="Reasoning-Capabilities"><a href="#Reasoning-Capabilities" class="headerlink" title="Reasoning Capabilities"></a>Reasoning Capabilities</h2><h3 id="GSM8K"><a href="#GSM8K" class="headerlink" title="GSM8K"></a>GSM8K</h3><ul>
<li><strong>Description:</strong> A set of 8.5K grade-school math problems that require basic to intermediate math operations.</li>
<li><strong>Purpose:</strong> To test LLMs’ ability to work through multistep math problems.</li>
<li><strong>Relevance:</strong> Useful for assessing AI’s capability in solving basic mathematical problems, valuable in educational contexts.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.14168">Training Verifiers to Solve Math Word Problems</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/gsm8k">GSM8K Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="Discrete-Reasoning-Over-Paragraphs-DROP"><a href="#Discrete-Reasoning-Over-Paragraphs-DROP" class="headerlink" title="Discrete Reasoning Over Paragraphs (DROP)"></a>Discrete Reasoning Over Paragraphs (DROP)</h3><ul>
<li><strong>Description:</strong> An adversarially-created reading comprehension benchmark requiring models to navigate through references and execute operations like addition or sorting.</li>
<li><strong>Purpose:</strong> To evaluate the ability of models to understand complex texts and perform discrete operations.</li>
<li><strong>Relevance:</strong> Useful in advanced educational tools and text analysis systems requiring logical reasoning.</li>
<li><strong>Source</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.00161">DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs</a></li>
<li><strong>Resources:</strong>:<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/drop">DROP Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="Counterfactual-Reasoning-Assessment-CRASS"><a href="#Counterfactual-Reasoning-Assessment-CRASS" class="headerlink" title="Counterfactual Reasoning Assessment (CRASS)"></a>Counterfactual Reasoning Assessment (CRASS)</h3><ul>
<li><strong>Description:</strong> Evaluates counterfactual reasoning abilities of LLMs, focusing on “what if” scenarios.</li>
<li><strong>Purpose:</strong> To assess models’ ability to understand and reason about alternate scenarios based on given data.</li>
<li><strong>Relevance:</strong> Important for AI applications in strategic planning, decision-making, and scenario analysis.</li>
<li><strong>Source</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.11941">CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/apergo-ai/CRASS-data-set/tree/main">CRASS Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="Large-scale-ReAding-Comprehension-Dataset-From-Examinations-RACE"><a href="#Large-scale-ReAding-Comprehension-Dataset-From-Examinations-RACE" class="headerlink" title="Large-scale ReAding Comprehension Dataset From Examinations (RACE)"></a>Large-scale ReAding Comprehension Dataset From Examinations (RACE)</h3><ul>
<li><strong>Description:</strong> A set of reading comprehension questions derived from English exams given to Chinese students.</li>
<li><strong>Purpose:</strong> To test LLMs’ understanding of complex reading material and their ability to answer examination-level questions.</li>
<li><strong>Relevance:</strong> Useful in language learning applications and educational systems for exam preparation.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.04683">RACE: Large-scale ReAding Comprehension Dataset From Examinations</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~glai1/data/race/">RAC Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="Big-Bench-Hard-BBH"><a href="#Big-Bench-Hard-BBH" class="headerlink" title="Big-Bench Hard (BBH)"></a>Big-Bench Hard (BBH)</h3><ul>
<li><strong>Description:</strong> A subset of BIG-Bench focusing on the most challenging tasks requiring multi-step reasoning.</li>
<li><strong>Purpose:</strong> To challenge LLMs with complex tasks demanding advanced reasoning skills.</li>
<li><strong>Relevance:</strong> Important for evaluating the upper limits of AI capabilities in complex reasoning and problem-solving.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.09261">Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/suzgunmirac/BIG-Bench-Hard">BIG-Bench-Hard GitHub: Dataset and Prompts</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/lukaemon/bbh">BBH Dataset: HuggingFace</a></li>
</ul>
</li>
</ul>
<h3 id="AGIEval"><a href="#AGIEval" class="headerlink" title="AGIEval"></a>AGIEval</h3><ul>
<li><strong>Description:</strong> A collection of standardized tests, including GRE, GMAT, SAT, LSAT, and civil service exams.</li>
<li><strong>Purpose:</strong> To evaluate LLMs’ reasoning abilities and problem-solving skills across various academic and professional scenarios.</li>
<li><strong>Relevance:</strong> Useful for assessing AI capabilities in standardized testing and professional qualification contexts.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.06364">AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ruixiangcui/AGIEval/tree/main">AGIEval Github: Dataset and Prompts</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets?search=AGIEval">AGIEval Datasets: HuggingFace</a></li>
</ul>
</li>
</ul>
<h3 id="BoolQ"><a href="#BoolQ" class="headerlink" title="BoolQ"></a>BoolQ</h3><ul>
<li><strong>Description:</strong> A collection of over 15,000 real yes&#x2F;no questions from Google searches, paired with Wikipedia passages.</li>
<li><strong>Purpose:</strong> To test the ability of LLMs to infer correct answers from contextual information that may not be explicit.</li>
<li><strong>Relevance:</strong> Crucial for question-answering systems and knowledge-based AI applications where accurate inference is key.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.10044">BoolQ: Exploring the Surprising Difficulty of Natural Yes&#x2F;No Questions</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/boolq">BoolQ Dataset: HuggingFace</a></li>
</ul>
</li>
</ul>
<h2 id="Multi-Turn-Open-Ended-Conversations"><a href="#Multi-Turn-Open-Ended-Conversations" class="headerlink" title="Multi Turn Open Ended Conversations"></a>Multi Turn Open Ended Conversations</h2><h3 id="MT-bench"><a href="#MT-bench" class="headerlink" title="MT-bench"></a>MT-bench</h3><ul>
<li><strong>Description:</strong> Tailored for evaluating the proficiency of chat assistants in sustaining multi-turn conversations.</li>
<li><strong>Purpose:</strong> To test the ability of models to engage in coherent and contextually relevant dialogues over multiple turns.</li>
<li><strong>Relevance:</strong> Essential for developing sophisticated conversational agents and chatbots.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/lmsys/mt_bench_human_judgments">MT-bench Human Annotation Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="Question-Answering-in-Context-QuAC"><a href="#Question-Answering-in-Context-QuAC" class="headerlink" title="Question Answering in Context (QuAC)"></a>Question Answering in Context (QuAC)</h3><ul>
<li><strong>Description:</strong> Features 14,000 dialogues with 100,000 question-answer pairs, simulating student-teacher interactions.</li>
<li><strong>Purpose:</strong> To challenge LLMs with context-dependent, sometimes unanswerable questions within dialogues.</li>
<li><strong>Relevance:</strong> Useful for conversational AI, educational software, and context-aware information systems.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.07036">QuAC : Question Answering in Context</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://quac.ai/">QuAC Homepage and Dataset</a></li>
</ul>
</li>
</ul>
<h2 id="Grounding-and-Abstractive-Summarization"><a href="#Grounding-and-Abstractive-Summarization" class="headerlink" title="Grounding and Abstractive Summarization"></a>Grounding and Abstractive Summarization</h2><h3 id="Ambient-Clinical-Intelligence-Benchmark-ACI-BENCH"><a href="#Ambient-Clinical-Intelligence-Benchmark-ACI-BENCH" class="headerlink" title="Ambient Clinical Intelligence Benchmark (ACI-BENCH)"></a>Ambient Clinical Intelligence Benchmark (ACI-BENCH)</h3><ul>
<li><strong>Description:</strong> Contains full doctor-patient conversations and associated clinical notes from various medical domains.</li>
<li><strong>Purpose:</strong> To challenge models to accurately generate clinical notes based on conversational data.</li>
<li><strong>Relevance:</strong> Vital for AI applications in healthcare, especially in automated documentation and medical analysis.</li>
<li><strong>Source:</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.02022">ACI-BENCH: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation</a></li>
<li><strong>Resources:</strong>:<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/wyim/aci-bench">ACI-BENCH Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="MAchine-Reading-COmprehension-Dataset-MS-MARCO"><a href="#MAchine-Reading-COmprehension-Dataset-MS-MARCO" class="headerlink" title="MAchine Reading COmprehension Dataset (MS-MARCO)"></a>MAchine Reading COmprehension Dataset (MS-MARCO)</h3><ul>
<li><strong>Description:</strong> A large-scale collection of natural language questions and answers derived from real web queries.</li>
<li><strong>Purpose:</strong> To test the ability of models to accurately understand and respond to real-world queries.</li>
<li><strong>Relevance:</strong> Crucial for search engines, question-answering systems, and other consumer-facing AI applications.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.09268">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ms_marco">MS-MARCO Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="Query-based-Multi-domain-Meeting-Summarization-QMSum"><a href="#Query-based-Multi-domain-Meeting-Summarization-QMSum" class="headerlink" title="Query-based Multi-domain Meeting Summarization (QMSum)"></a>Query-based Multi-domain Meeting Summarization (QMSum)</h3><ul>
<li><strong>Description:</strong> A benchmark for summarizing relevant spans of meetings in response to specific queries.</li>
<li><strong>Purpose:</strong> To evaluate the ability of models to extract and summarize important information from meeting content.</li>
<li><strong>Relevance:</strong> Useful for business intelligence tools, meeting analysis applications, and automated summarization systems.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.05938">QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/Yale-LILY/QMSum">QMSum Dataset</a></li>
</ul>
</li>
</ul>
<h3 id="Physical-Interaction-Question-Answering-PIQA"><a href="#Physical-Interaction-Question-Answering-PIQA" class="headerlink" title="Physical Interaction: Question Answering (PIQA)"></a>Physical Interaction: Question Answering (PIQA)</h3><ul>
<li><strong>Description:</strong> Tests knowledge and understanding of the physical world through hypothetical scenarios and solutions.</li>
<li><strong>Purpose:</strong> To measure the model’s capability in handling physical interaction scenarios.</li>
<li><strong>Relevance:</strong> Important for AI applications in robotics, physical simulations, and practical problem-solving systems.</li>
<li><strong>Source:</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.11641">PIQA: Reasoning about Physical Commonsense in Natural Language</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ybisk/ybisk.github.io/tree/master/piqa">PIQA Dataset: GitHub</a></li>
</ul>
</li>
</ul>
<h2 id="Content-Moderation-and-Narrative-Control"><a href="#Content-Moderation-and-Narrative-Control" class="headerlink" title="Content Moderation and Narrative Control"></a>Content Moderation and Narrative Control</h2><h3 id="ToxiGen"><a href="#ToxiGen" class="headerlink" title="ToxiGen"></a>ToxiGen</h3><ul>
<li><strong>Description:</strong> A dataset of toxic and benign statements about minority groups, focusing on implicit hate speech.</li>
<li><strong>Purpose:</strong> To test a model’s ability to both identify and avoid generating toxic content.</li>
<li><strong>Relevance:</strong> Crucial for content moderation systems, community management, and AI ethics research.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.09509">ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/microsoft/TOXIGEN/tree/main">TOXIGEN Code and Prompts: GitHub</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/skg/toxigen-data">TOXIGEN Dataset: HuggingFace</a></li>
</ul>
</li>
</ul>
<h3 id="Helpfulness-Honesty-Harmlessness-HHH"><a href="#Helpfulness-Honesty-Harmlessness-HHH" class="headerlink" title="Helpfulness, Honesty, Harmlessness (HHH)"></a>Helpfulness, Honesty, Harmlessness (HHH)</h3><ul>
<li><strong>Description:</strong> Evaluates language models’ alignment with ethical standards such as helpfulness, honesty, and harmlessness.</li>
<li><strong>Purpose:</strong> To assess the ethical responses of models in interaction scenarios.</li>
<li><strong>Relevance:</strong> Vital for ensuring AI systems promote positive interactions and adhere to ethical standards.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.00861">A General Language Assistant as a Laboratory for Alignment</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/anthropics/hh-rlhf">HH-RLHF Datasets: GitHub</a></li>
<li>Related Research:<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.05862">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.07858">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="TruthfulQA"><a href="#TruthfulQA" class="headerlink" title="TruthfulQA"></a>TruthfulQA</h3><ul>
<li><strong>Description:</strong> A benchmark for evaluating the truthfulness of LLMs in generating answers to questions prone to false beliefs and biases.</li>
<li><strong>Purpose:</strong> To test the ability of models to provide accurate and unbiased information.</li>
<li><strong>Relevance:</strong> Important for AI systems where delivering accurate and unbiased information is critical, such as in educational or advisory roles.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2109.07958v2">TruthfulQA: Measuring How Models Mimic Human Falsehoods</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/sylinrl/TruthfulQA">TruthfulQA Dataset: GitHub</a></li>
</ul>
</li>
</ul>
<h3 id="Responsible-AI-RAI"><a href="#Responsible-AI-RAI" class="headerlink" title="Responsible AI (RAI)"></a>Responsible AI (RAI)</h3><ul>
<li><strong>Description:</strong> A framework for evaluating the safety of chat-optimized models in conversational settings.</li>
<li><strong>Purpose:</strong> To assess potential harmful content, IP leakage, and security breaches in AI-driven conversations.</li>
<li><strong>Relevance:</strong> Crucial for developing safe and secure conversational AI applications, particularly in sensitive domains.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.17750">A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications</a></li>
</ul>
<h2 id="Coding-Capabilities"><a href="#Coding-Capabilities" class="headerlink" title="Coding Capabilities"></a>Coding Capabilities</h2><h3 id="CodeXGLUE"><a href="#CodeXGLUE" class="headerlink" title="CodeXGLUE"></a>CodeXGLUE</h3><ul>
<li><strong>Description:</strong> Evaluates LLMs’ ability to understand and work with code across various tasks like code completion and translation.</li>
<li><strong>Purpose:</strong> To assess code intelligence, including understanding, fixing, and explaining code.</li>
<li><strong>Relevance:</strong> Essential for applications in software development, code analysis, and technical documentation.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.04664">CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</a></li>
<li><strong>Reference:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/microsoft/CodeXGLUE">CodeXGLUE Dataset: GitHub</a></li>
</ul>
</li>
</ul>
<h3 id="HumanEval"><a href="#HumanEval" class="headerlink" title="HumanEval"></a>HumanEval</h3><ul>
<li><strong>Description:</strong> Contains programming challenges for evaluating LLMs’ ability to write functional code based on instructions.</li>
<li><strong>Purpose:</strong> To test the generation of correct and efficient code from given requirements.</li>
<li><strong>Relevance:</strong> Important for automated code generation tools, programming assistants, and coding education platforms.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/openai/human-eval">HumanEval Dataset: GitHub</a></li>
</ul>
</li>
</ul>
<h3 id="Mostly-Basic-Python-Programming-MBPP"><a href="#Mostly-Basic-Python-Programming-MBPP" class="headerlink" title="Mostly Basic Python Programming (MBPP)"></a>Mostly Basic Python Programming (MBPP)</h3><ul>
<li><strong>Description:</strong> Includes 1,000 Python programming problems suitable for entry-level programmers.</li>
<li><strong>Purpose:</strong> To evaluate proficiency in solving basic programming tasks and understanding of Python.</li>
<li><strong>Relevance:</strong> Useful for beginner-level coding education, automated code generation, and entry-level programming testing.</li>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2108.07732">Program Synthesis with Large Language Models</a></li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/mbpp">MBPP Dataset: HuggingFace</a></li>
</ul>
</li>
</ul>
<h2 id="LLM-Assisted-Evaluation"><a href="#LLM-Assisted-Evaluation" class="headerlink" title="LLM-Assisted Evaluation"></a>LLM-Assisted Evaluation</h2><h3 id="LLM-Judge"><a href="#LLM-Judge" class="headerlink" title="LLM Judge"></a>LLM Judge</h3><ul>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a></li>
<li><strong>Abstract:</strong> Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at this https URL.</li>
<li><strong>Insights:</strong><ul>
<li>Use MT-bench questions and prompts to evaluate your models with LLM-as-a-judge. MT-bench is a set of challenging multi-turn open-ended questions for evaluating chat assistants. To automate the evaluation process, we prompt strong LLMs like GPT-4 to act as judges and assess the quality of the models’ responses.</li>
</ul>
</li>
<li><strong>Resources:</strong><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge">LLM Judge GitHub</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">MT-bench and Arena Elo Leaderboard</a></li>
<li><a target="_blank" rel="noopener" href="https://chat.lmsys.org/?arena">Chatbot Arena</a></li>
<li>LLM Judge Datasets:<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m">LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/lmsys/chatbot_arena_conversations">Chatbot Arena Conversation Dataset</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/lmsys/mt_bench_human_judgments">MT-bench Human Annotation Dataset</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="LLM-Eval"><a href="#LLM-Eval" class="headerlink" title="LLM-Eval"></a>LLM-Eval</h3><ul>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.13711">Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models</a></li>
<li><strong>Abstract:</strong> We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.</li>
<li><strong>Insights:</strong><ul>
<li>Top-shelve LLM (e.g. GPT4, Claude) correlate better with human score than metric-based eval measures.</li>
</ul>
</li>
</ul>
<h3 id="JudgeLM"><a href="#JudgeLM" class="headerlink" title="JudgeLM"></a>JudgeLM</h3><ul>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.17631">JudgeLM: Fine-tuned Large Language Models are Scalable Judges</a></li>
<li><strong>Abstract:</strong> Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge’s performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.</li>
<li><strong>Insights:</strong><ul>
<li>Relatively small models (e.g 7b models) can be fine-tuned to be reliable judges of other models.</li>
</ul>
</li>
</ul>
<h3 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h3><ul>
<li><strong>Source:</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2310.08491">Prometheus: Inducing Fine-grained Evaluation Capability in Language Models</a></li>
<li><strong>Abstract:</strong> Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus’s capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment &amp; MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model.</li>
<li><strong>Insights:</strong><ul>
<li>A scoring  rubric and a reference answer vastly improve correlation with human scores.</li>
</ul>
</li>
</ul>
<h2 id="Industry-Resources"><a href="#Industry-Resources" class="headerlink" title="Industry Resources"></a>Industry Resources</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.latent.space/p/benchmarks-201?publication_id=1084089&post_id=146497374&isFreemail=true&r=53qrp&triedRedirect=true">Latent Space - Benchmarks 201: Why Leaderboards &gt; Arenas &gt;&gt; LLM-as-Judge</a><ul>
<li><strong>Summary:</strong><ul>
<li>The OpenLLM Leaderboard, maintained by Clémentine Fourrier, is a standardized and reproducible way to evaluate language models’ performance.</li>
<li>The leaderboard initially gained popularity in summer 2023 and has had over 2 million unique visitors and 300,000 active community members.</li>
<li>The recent update to the leaderboard (v2) includes six benchmarks to address model overfitting and to provide more room for improved performance.</li>
<li>LLMs are not recommended as judges due to issues like mode collapse and positional bias.</li>
<li>If LLMs must be used as judges, open LLMs like Prometheus or JudgeLM are suggested for reproducibility.</li>
<li>The LMSys Arena is another platform for AI engineers, but its rankings are not reproducible and may not accurately reflect model capabilities.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>ref: <a target="_blank" rel="noopener" href="https://github.com/leobeeson/llm_benchmarks/blob/master/README.md%60">https://github.com/leobeeson/llm_benchmarks/blob/master/README.md`</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://88daxiong.github.io">daxiong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://88daxiong.github.io/2024/09/23/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BLLM%E7%9A%84%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86/">https://88daxiong.github.io/2024/09/23/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BLLM%E7%9A%84%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://88daxiong.github.io" target="_blank">daxiong's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/llm/">llm</a><a class="post-meta__tags" href="/tags/%E8%AF%84%E6%B5%8B/">评测</a></div><div class="post_share"><div class="social-share" data-image="/image/bald_eagle.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/09/19/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/" title="mysql数据库备份与恢复"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">mysql数据库备份与恢复</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/image/bald_eagle.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">daxiong</div><div class="author-info__description">Stay Hungry, Stay Foolish</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/88daxiong"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">缘起性空</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#llm-benchmarks"><span class="toc-number">1.</span> <span class="toc-text">llm_benchmarks</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Knowledge-and-Language-Understanding"><span class="toc-number">1.1.</span> <span class="toc-text">Knowledge and Language Understanding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Massive-Multitask-Language-Understanding-MMLU"><span class="toc-number">1.1.1.</span> <span class="toc-text">Massive Multitask Language Understanding (MMLU)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AI2-Reasoning-Challenge-ARC"><span class="toc-number">1.1.2.</span> <span class="toc-text">AI2 Reasoning Challenge (ARC)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#General-Language-Understanding-Evaluation-GLUE"><span class="toc-number">1.1.3.</span> <span class="toc-text">General Language Understanding Evaluation (GLUE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Natural-Questions"><span class="toc-number">1.1.4.</span> <span class="toc-text">Natural Questions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LAnguage-Modelling-Broadened-to-Account-for-Discourse-Aspects-LAMBADA"><span class="toc-number">1.1.5.</span> <span class="toc-text">LAnguage Modelling Broadened to Account for Discourse Aspects (LAMBADA)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HellaSwag"><span class="toc-number">1.1.6.</span> <span class="toc-text">HellaSwag</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Genre-Natural-Language-Inference-MultiNLI"><span class="toc-number">1.1.7.</span> <span class="toc-text">Multi-Genre Natural Language Inference (MultiNLI)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SuperGLUE"><span class="toc-number">1.1.8.</span> <span class="toc-text">SuperGLUE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TriviaQA"><span class="toc-number">1.1.9.</span> <span class="toc-text">TriviaQA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WinoGrande"><span class="toc-number">1.1.10.</span> <span class="toc-text">WinoGrande</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SciQ"><span class="toc-number">1.1.11.</span> <span class="toc-text">SciQ</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reasoning-Capabilities"><span class="toc-number">1.2.</span> <span class="toc-text">Reasoning Capabilities</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GSM8K"><span class="toc-number">1.2.1.</span> <span class="toc-text">GSM8K</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Discrete-Reasoning-Over-Paragraphs-DROP"><span class="toc-number">1.2.2.</span> <span class="toc-text">Discrete Reasoning Over Paragraphs (DROP)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Counterfactual-Reasoning-Assessment-CRASS"><span class="toc-number">1.2.3.</span> <span class="toc-text">Counterfactual Reasoning Assessment (CRASS)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Large-scale-ReAding-Comprehension-Dataset-From-Examinations-RACE"><span class="toc-number">1.2.4.</span> <span class="toc-text">Large-scale ReAding Comprehension Dataset From Examinations (RACE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Big-Bench-Hard-BBH"><span class="toc-number">1.2.5.</span> <span class="toc-text">Big-Bench Hard (BBH)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AGIEval"><span class="toc-number">1.2.6.</span> <span class="toc-text">AGIEval</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BoolQ"><span class="toc-number">1.2.7.</span> <span class="toc-text">BoolQ</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-Turn-Open-Ended-Conversations"><span class="toc-number">1.3.</span> <span class="toc-text">Multi Turn Open Ended Conversations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MT-bench"><span class="toc-number">1.3.1.</span> <span class="toc-text">MT-bench</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Question-Answering-in-Context-QuAC"><span class="toc-number">1.3.2.</span> <span class="toc-text">Question Answering in Context (QuAC)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Grounding-and-Abstractive-Summarization"><span class="toc-number">1.4.</span> <span class="toc-text">Grounding and Abstractive Summarization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Ambient-Clinical-Intelligence-Benchmark-ACI-BENCH"><span class="toc-number">1.4.1.</span> <span class="toc-text">Ambient Clinical Intelligence Benchmark (ACI-BENCH)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MAchine-Reading-COmprehension-Dataset-MS-MARCO"><span class="toc-number">1.4.2.</span> <span class="toc-text">MAchine Reading COmprehension Dataset (MS-MARCO)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Query-based-Multi-domain-Meeting-Summarization-QMSum"><span class="toc-number">1.4.3.</span> <span class="toc-text">Query-based Multi-domain Meeting Summarization (QMSum)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Physical-Interaction-Question-Answering-PIQA"><span class="toc-number">1.4.4.</span> <span class="toc-text">Physical Interaction: Question Answering (PIQA)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Content-Moderation-and-Narrative-Control"><span class="toc-number">1.5.</span> <span class="toc-text">Content Moderation and Narrative Control</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ToxiGen"><span class="toc-number">1.5.1.</span> <span class="toc-text">ToxiGen</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Helpfulness-Honesty-Harmlessness-HHH"><span class="toc-number">1.5.2.</span> <span class="toc-text">Helpfulness, Honesty, Harmlessness (HHH)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TruthfulQA"><span class="toc-number">1.5.3.</span> <span class="toc-text">TruthfulQA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Responsible-AI-RAI"><span class="toc-number">1.5.4.</span> <span class="toc-text">Responsible AI (RAI)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Coding-Capabilities"><span class="toc-number">1.6.</span> <span class="toc-text">Coding Capabilities</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CodeXGLUE"><span class="toc-number">1.6.1.</span> <span class="toc-text">CodeXGLUE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HumanEval"><span class="toc-number">1.6.2.</span> <span class="toc-text">HumanEval</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mostly-Basic-Python-Programming-MBPP"><span class="toc-number">1.6.3.</span> <span class="toc-text">Mostly Basic Python Programming (MBPP)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM-Assisted-Evaluation"><span class="toc-number">1.7.</span> <span class="toc-text">LLM-Assisted Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM-Judge"><span class="toc-number">1.7.1.</span> <span class="toc-text">LLM Judge</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM-Eval"><span class="toc-number">1.7.2.</span> <span class="toc-text">LLM-Eval</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JudgeLM"><span class="toc-number">1.7.3.</span> <span class="toc-text">JudgeLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Prometheus"><span class="toc-number">1.7.4.</span> <span class="toc-text">Prometheus</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Industry-Resources"><span class="toc-number">1.8.</span> <span class="toc-text">Industry Resources</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/23/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BLLM%E7%9A%84%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86/" title="大语言模型LLM的评测基准数据集">大语言模型LLM的评测基准数据集</a><time datetime="2024-09-23T02:30:15.000Z" title="发表于 2024-09-23 10:30:15">2024-09-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/19/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/" title="mysql数据库备份与恢复">mysql数据库备份与恢复</a><time datetime="2024-09-19T14:40:40.000Z" title="发表于 2024-09-19 22:40:40">2024-09-19</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By daxiong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>